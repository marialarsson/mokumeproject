{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d69351",
   "metadata": {},
   "source": [
    "# The Mokume Dataset and Inverse Texturing of Solid Wood (NCA Training)\n",
    "\n",
    "This notebook is a step by step guide to train an NCA model to create 3D wood textures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b4e00",
   "metadata": {},
   "source": [
    "## Utils and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eecd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import shutil\n",
    "import PIL\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, HTML, Markdown, clear_output, display\n",
    "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
    "import moviepy.editor as mvp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# from COMMON.data_utils import get_unfolded_image\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_device('cuda:0')\n",
    "prec = torch.float16\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
    "\n",
    "class VideoWriter:\n",
    "    def __init__(self, filename='_autoplay.mp4', fps=30.0, **kw):\n",
    "        self.writer = None\n",
    "        self.params = dict(filename=filename, fps=fps, **kw)\n",
    "\n",
    "    def add(self, img):\n",
    "        img = np.asarray(img)\n",
    "        if self.writer is None:\n",
    "            h, w = img.shape[:2]\n",
    "            self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
    "        if img.dtype in [np.float32, np.float64]:\n",
    "            img = np.uint8(img.clip(0, 1) * 255)\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.repeat(img[..., None], 3, -1)\n",
    "        self.writer.write_frame(img)\n",
    "\n",
    "    def close(self):\n",
    "        if self.writer:\n",
    "            self.writer.close()\n",
    "\n",
    "    def show(self, **kw):\n",
    "        self.close()\n",
    "        fn = self.params['filename']\n",
    "        display(mvp.ipython_display(fn, **kw))\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *kw):\n",
    "        self.close()\n",
    "\n",
    "        \n",
    "def get_unfolded_image(imgs, black_bg=False): #false before, does it cause any problems?\n",
    "\n",
    "    all_imgs = []\n",
    "\n",
    "    for i in range(6):\n",
    "        if i<len(imgs):\n",
    "            img = np.copy(imgs[i])\n",
    "            if img.dtype!=np.uint8:\n",
    "                img = (255.0*img).astype(np.uint8)\n",
    "            all_imgs.append(img)\n",
    "        else:\n",
    "            all_imgs.append(np.ones(imgs[0].shape, dtype=np.uint8)*255)\n",
    "\n",
    "    depth = all_imgs[1].shape[0]\n",
    "    shape_base = list(all_imgs[0].shape)\n",
    "    shape_base[0] = depth\n",
    "    shape_f  = tuple(shape_base)\n",
    "    shape_ce = list(shape_base)\n",
    "    shape_ce[1] = depth\n",
    "    shape_ce = tuple(shape_ce)\n",
    "\n",
    "    if black_bg: \n",
    "        empty_img_ce = np.zeros(shape_ce, dtype=np.uint8)\n",
    "        empty_img_f =  np.zeros(shape_f, dtype=np.uint8)\n",
    "        \n",
    "    else: #white bg         \n",
    "        empty_img_ce = np.ones(shape_ce, dtype=np.uint8)*255\n",
    "        empty_img_f = np.ones(shape_f, dtype=np.uint8)*255\n",
    "    \n",
    "    col1 = np.vstack(((empty_img_ce, all_imgs[2], empty_img_ce)))\n",
    "    col2 = np.vstack(((all_imgs[1], all_imgs[0], all_imgs[3])))\n",
    "    col3 = np.vstack(((empty_img_ce, all_imgs[4], empty_img_ce)))\n",
    "    col4 = np.vstack(((empty_img_f, all_imgs[5], empty_img_f)))\n",
    "\n",
    "    unfolded_img = np.hstack((col1,col2,col3,col4))\n",
    "\n",
    "    return unfolded_img\n",
    "        \n",
    "def to_pil_image(x):\n",
    "    \"\"\"\n",
    "    x: tensor with shape [b, 3, h, w] or [b, h, w]\n",
    "    \"\"\"\n",
    "    if x.ndim == 3:\n",
    "        x = x.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    x = (x * 255).clamp(0, 255).to(torch.uint8)\n",
    "    x = x.cpu().numpy()\n",
    "    x = np.hstack(x)\n",
    "    return PIL.Image.fromarray(x)\n",
    "\n",
    "def get_outer_faces(x):\n",
    "    \"\"\"\n",
    "    x: tensor with shape [c, h, w, d] or [h, w, d]\n",
    "    Returns the outer faces of the 3D tensor. [b, c, h, w] or [b, h, w]\n",
    "    \"\"\"\n",
    "    return torch.stack([\n",
    "        x[..., 0, :, :], x[..., -1, :, :], # C, E\n",
    "        x[..., :, 0, :], x[..., :, -1, :], # D, B\n",
    "        x[..., :, :, 0], x[..., :, :, -1], # F, A\n",
    "    ], dim=0)\n",
    "\n",
    "def reorder_faces(faces, order=\"ABCDEF\"):\n",
    "    \"\"\"\n",
    "    faces: [6, c, h, w] \n",
    "    external faces of the cube in A B C D E F order\n",
    "    \n",
    "    returns: external_faces [6, c, h, w]\n",
    "    \"\"\"\n",
    "    assert order in [\"ABCDEF\", \"ZYX\"] \n",
    "    # Reorder external faces to be compatible with the 3D volumetric cube \n",
    "    if order == \"ABCDEF\":\n",
    "        return torch.stack((\n",
    "            torch.rot90(faces[5], -3, (1, 2)),  # A, X=+1\n",
    "            torch.rot90(torch.flip(faces[3], (1, )), -1, (1, 2)), # B, Y=+1\n",
    "            torch.flip(faces[0], (1,)), # C, Z = -1\n",
    "            torch.rot90(faces[2], -3, (1, 2)),  # D, Y=-1\n",
    "            torch.flip(faces[1], (1, 2,)),  # E, Z=+1\n",
    "            torch.rot90(torch.flip(faces[4], (2, )), -1, (1, 2)) # F, X=-1\n",
    "        ))\n",
    "        \n",
    "    elif order == \"ZYX\":\n",
    "        return torch.stack((\n",
    "            torch.flip(faces[2], (1,)),  # C, Z=-1\n",
    "            torch.flip(faces[4], (1, 2,)),  # E, Z=+1\n",
    "            torch.rot90(faces[3], 3, (1, 2)),  # D, Y=-1\n",
    "            torch.flip(torch.rot90(faces[1], 1, (1, 2)), (1,)),  # B, Y=+1\n",
    "            torch.flip(torch.rot90(faces[5], 1, (1, 2)), (2,)),  # F, X=-1\n",
    "            torch.rot90(faces[0], 3, (1, 2)),  # A, X=+1\n",
    "        ))\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2fa9b",
   "metadata": {},
   "source": [
    "## VGG-Based Style Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelaxedOTLoss(torch.nn.Module):\n",
    "    \"\"\"Loss function proposed in --> https://arxiv.org/abs/1904.12785\"\"\"\n",
    "    \"\"\"Code taken from --> https://arxiv.org/pdf/2404.06279\"\"\"\n",
    "    def __init__(self, vgg, target_image, n_samples=1024):\n",
    "        super().__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.vgg = vgg\n",
    "        with torch.no_grad():\n",
    "            self.target_features = self.get_vgg_features(target_image)\n",
    "\n",
    "    def get_vgg_features(self, imgs):\n",
    "        style_layers = [1, 6, 11, 18, 25]\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
    "        std = torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n",
    "        x = (imgs - mean) / std\n",
    "        b, c, h, w = x.shape\n",
    "        features = []\n",
    "        for i, layer in enumerate(self.vgg[:max(style_layers) + 1]):\n",
    "            x = layer(x)\n",
    "            if i in style_layers:\n",
    "                b, c, h, w = x.shape\n",
    "                features.append(x.reshape(b, c, h * w))\n",
    "        return features\n",
    "\n",
    "    @staticmethod\n",
    "    def pairwise_distances_cos(x, y):\n",
    "        x_norm = torch.norm(x, dim=2, keepdim=True)  # (b, n, 1)\n",
    "        y_t = y.transpose(1, 2)  # (b, c, m) (m may be different from n)\n",
    "        y_norm = torch.norm(y_t, dim=1, keepdim=True)  # (b, 1, m)\n",
    "        dist = 1. - torch.matmul(x, y_t) / (x_norm * y_norm + 1e-10)  # (b, n, m)\n",
    "        return dist\n",
    "\n",
    "    @staticmethod\n",
    "    def style_loss(x, y):\n",
    "        pairwise_distance = RelaxedOTLoss.pairwise_distances_cos(x, y)\n",
    "        m1, m1_inds = pairwise_distance.min(1)\n",
    "        m2, m2_inds = pairwise_distance.min(2)\n",
    "        remd = torch.max(m1.mean(dim=1), m2.mean(dim=1))\n",
    "        return remd\n",
    "\n",
    "    @staticmethod\n",
    "    def moment_loss(x, y):\n",
    "        mu_x, mu_y = torch.mean(x, 1, keepdim=True), torch.mean(y, 1, keepdim=True)\n",
    "        mu_diff = torch.abs(mu_x - mu_y).mean(dim=(1, 2))\n",
    "\n",
    "        x_c, y_c = x - mu_x, y - mu_y\n",
    "        x_cov = torch.matmul(x_c.transpose(1, 2), x_c) / (x.shape[1] - 1)\n",
    "        y_cov = torch.matmul(y_c.transpose(1, 2), y_c) / (y.shape[1] - 1)\n",
    "\n",
    "        cov_diff = torch.abs(x_cov - y_cov).mean(dim=(1, 2))\n",
    "        return mu_diff + cov_diff\n",
    "\n",
    "    def forward(self, generated_image):\n",
    "        loss = 0.0\n",
    "        generated_features = self.get_vgg_features(generated_image)\n",
    "        # Iterate over the VGG layers\n",
    "        for x, y in zip(generated_features, self.target_features):\n",
    "            (b_x, c, n_x), (b_y, _, n_y) = x.shape, y.shape\n",
    "            n_samples = min(n_x, n_y, self.n_samples)\n",
    "            indices_x = torch.argsort(torch.rand(b_x, 1, n_x, device=x.device), dim=-1)[..., :n_samples]\n",
    "            x = x.gather(-1, indices_x.expand(b_x, c, n_samples))\n",
    "            indices_y = torch.argsort(torch.rand(b_y, 1, n_y, device=y.device), dim=-1)[..., :n_samples]\n",
    "            y = y.gather(-1, indices_y.expand(b_y, c, n_samples))\n",
    "            x, y = x.transpose(1, 2), y.transpose(1, 2)  # (b, n_samples, c)\n",
    "            loss += self.style_loss(x, y) + self.moment_loss(x, y)\n",
    "\n",
    "        return loss.mean()\n",
    "    \n",
    "vgg = models.vgg16(weights='IMAGENET1K_V1').features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b84173",
   "metadata": {},
   "source": [
    "## Volumetric NCA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_emb_3D(D):\n",
    "    xs = torch.arange(D, dtype=prec) / D\n",
    "    xs = 2.0 * (xs - 0.5 + 0.5 / D)\n",
    "    xs, ys, zs = xs[None, :, None, None], xs[None, None, :, None], xs[None, None, None, :]\n",
    "    grid = torch.zeros((3, D, D, D), dtype=prec)\n",
    "    grid[0:1], grid[1:2], grid[2:3] = xs, ys, zs\n",
    "    return grid\n",
    "\n",
    "def depthwise_conv(x, filters):\n",
    "    \"\"\"filters: [filter_n, h, w]\"\"\"\n",
    "    b, ch, h, w, d = x.shape\n",
    "    y = x.reshape(b * ch, 1, h, w, d)\n",
    "    y = torch.nn.functional.pad(y, [1, 1, 1, 1, 1, 1], \"replicate\")\n",
    "    y = torch.nn.functional.conv3d(y, filters[:, None])\n",
    "    return y.reshape(b, -1, h, w, d)\n",
    "\n",
    "class VolumeNCA(torch.nn.Module):\n",
    "    def __init__(self, chn=12, fc_dim=128, noise_level=0.1, pemb=True):\n",
    "        super().__init__()\n",
    "        self.chn = chn\n",
    "        self.register_buffer(\"noise_level\", torch.tensor([noise_level], dtype=prec))\n",
    "\n",
    "        input_dim = (self.chn + 2) * 5\n",
    "        self.pemb = pemb\n",
    "        if pemb:\n",
    "            input_dim += 3\n",
    "        self.w1 = torch.nn.Conv3d(input_dim, fc_dim, 1, bias=True)\n",
    "        self.w2 = torch.nn.Conv3d(fc_dim, self.chn, 1)\n",
    "\n",
    "        torch.nn.init.xavier_normal_(self.w1.weight, gain=0.2)\n",
    "        torch.nn.init.zeros_(self.w2.weight)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            delta_one = torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 0.0, 2.0], [-1.0, 0.0, 1.0]], dtype=prec)\n",
    "            delta_two = torch.tensor([[-2.0, 0.0, 2.0], [-4.0, 0.0, 4.0], [-2.0, 0.0, 2.0]], dtype=prec)\n",
    "            sobel_z = torch.stack([delta_one, delta_two, delta_one]) / 4.0\n",
    "            sobel_y = sobel_z.permute(0, 2, 1)\n",
    "            sobel_x = sobel_z.permute(2, 1, 0)\n",
    "\n",
    "            lap1 = torch.tensor([[2.0, 3.0, 2.0], [3.0, 6.0, 3.0], [2.0, 3.0, 2.0]], dtype=prec)\n",
    "            lap2 = torch.tensor([[3.0, 6.0, 3.0], [6.0, -88.0, 6.0], [3.0, 6.0, 3.0]], dtype=prec)\n",
    "            lap = torch.stack([lap1, lap2, lap1])\n",
    "            #             lap = (lap / 26.0)\n",
    "            lap = lap / 8.0\n",
    "\n",
    "            ident = torch.zeros(3, 3, 3, dtype=prec)\n",
    "            ident[1, 1, 1] = 1.0\n",
    "\n",
    "            self.filters = torch.stack([ident, sobel_x, sobel_y, sobel_z, lap])\n",
    "\n",
    "    def forward(self, s, cond, p=None):\n",
    "        z = depthwise_conv(torch.cat([s, cond], dim=1), self.filters)  # [b, 5 * chn, h, w]\n",
    "        if p is not None:\n",
    "            z = torch.cat([z, p], dim=1)\n",
    "        delta_s = self.w2(torch.relu(self.w1(z)))\n",
    "        return s + delta_s\n",
    "\n",
    "    def seed(self, n, h=128, w=128, d=128):\n",
    "        with torch.no_grad():\n",
    "            return (torch.rand(n, self.chn, h, w, d, dtype=prec) - 0.5) * self.noise_level\n",
    "\n",
    "\n",
    "def to_rgb(s):\n",
    "    return s[..., :3, :, :, :] + 0.5\n",
    "\n",
    "\n",
    "def rgb_delta(s):\n",
    "    return s[..., :3, :, :, :]\n",
    "\n",
    "print('Number of NCA parameters:', sum(p.numel() for p in VolumeNCA().parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953811e6",
   "metadata": {},
   "source": [
    "## Load and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b18dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name = \"H11\"\n",
    "res = 64\n",
    "with torch.no_grad():\n",
    "    grid = get_pos_emb_3D(res)\n",
    "    col = np.load(f'Samples/{sample_name}/col_cube.npz')['arr_0']\n",
    "    gf = np.load(f'Samples/{sample_name}/gf_cube.npz')['arr_0']\n",
    "    \n",
    "    external = np.stack([PIL.Image.open(f\"Samples/{sample_name}/{face}_col.png\") for face in \"ABCDEF\"]) / 255.0 # Unmuted colors\n",
    "\n",
    "    col = torch.tensor(col, dtype=prec).permute(3, 0, 1, 2).contiguous()\n",
    "    col = F.interpolate(col.unsqueeze(0), size=(res, res, res), mode='trilinear', align_corners=False).squeeze(0) # [3, res, res, res]\n",
    "\n",
    "    gf = torch.tensor(gf, dtype=prec)\n",
    "    gf = F.interpolate(gf[None, None, :], size=(res, res, res), mode='trilinear', align_corners=False)[0, 0] # [res, res, res]\n",
    "\n",
    "    external = torch.tensor(external, dtype=prec)\n",
    "    external = F.interpolate(external.permute(0, 3, 1, 2), size=(res, res), mode='bilinear', align_corners=False)\n",
    "\n",
    "    col_cond = col.unsqueeze(0).mean(dim=1) - 0.5  # instead of * 2 - 1 [1, res, res, res]\n",
    "    gf_cond = gf.unsqueeze(0) - 0.5  # instead of * 2 - 1 # [1, res, res, res]\n",
    "    cond = torch.stack([col_cond, gf_cond], dim=1) # [2, res, res, res]\n",
    "\n",
    "    external_faces = reorder_faces(external, \"ZYX\")\n",
    "    loss_fn = RelaxedOTLoss(vgg, external_faces)\n",
    "\n",
    "    print(\"External Faces in ZYX order\")\n",
    "    to_pil_image(external_faces).show()\n",
    "    to_pil_image(get_outer_faces(col)).show()\n",
    "    to_pil_image(get_outer_faces(gf)).show()\n",
    "    \n",
    "\n",
    "    print(\"Faces in ABCDEF order\")\n",
    "    external_ABCDEF = reorder_faces(external_faces, \"ABCDEF\")\n",
    "    PIL.Image.fromarray(get_unfolded_image(external_ABCDEF.permute(0, 2, 3, 1).cpu().numpy())).show()\n",
    "    \n",
    "    col_ABCDEF = reorder_faces(get_outer_faces(col), \"ABCDEF\")\n",
    "    PIL.Image.fromarray(get_unfolded_image(col_ABCDEF.permute(0, 2, 3, 1).cpu().numpy())).show()\n",
    "\n",
    "    gf_ABCDEF = reorder_faces(get_outer_faces(gf).unsqueeze(1).repeat(1, 3, 1, 1), \"ABCDEF\")\n",
    "    PIL.Image.fromarray(get_unfolded_image(gf_ABCDEF.permute(0, 2, 3, 1).cpu().numpy())).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173f24de",
   "metadata": {},
   "source": [
    "## Initialize and Train the NCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5accfbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VolumeNCA(chn=12, fc_dim=128, noise_level=0.1, pemb=True)\n",
    "opt = torch.optim.Adam(model.parameters(), 0.001, fused=True)\n",
    "lr_sched = torch.optim.lr_scheduler.MultiStepLR(opt, [700, 1200], 0.3)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "pool_size = 32  # Number of NCA states in pool\n",
    "batch_size = 1 # Batch size must be 1\n",
    "step_range = (8, 16)\n",
    "gradient_checkpoint = True\n",
    "loss_log = []\n",
    "with torch.no_grad():\n",
    "    pool = model.seed(pool_size, res, res, res)\n",
    "    \n",
    "model = VolumeNCA(chn=12, fc_dim=128, noise_level=0.1, pemb=True)\n",
    "opt = torch.optim.Adam(model.parameters(), 0.001, fused=True)\n",
    "lr_sched = torch.optim.lr_scheduler.MultiStepLR(opt, [700, 1200], 0.3)\n",
    "scaler = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561874a",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfca3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1500)):\n",
    "    step_n = np.random.randint(step_range[0], step_range[1])  # 32..96\n",
    "    batch_idx = np.random.choice(len(pool), batch_size, replace=False)\n",
    "    s = pool[batch_idx]\n",
    "    if epoch % 32 == 0:\n",
    "        s[:1] = model.seed(1, res, res, res)\n",
    "\n",
    "    if model.pemb:\n",
    "        pemb = grid[None, ...]\n",
    "    else:\n",
    "        pemb = None\n",
    "\n",
    "\n",
    "\n",
    "    with torch.autocast(device_type='cuda'):\n",
    "        if not gradient_checkpoint:\n",
    "            for k in range(step_n):\n",
    "                s = model(s, cond, pemb)\n",
    "        else:\n",
    "            model_forward = lambda x: model(x, cond, pemb)\n",
    "            s.requires_grad = True  # https://github.com/pytorch/pytorch/issues/42812\n",
    "            s = torch.utils.checkpoint.checkpoint_sequential([model_forward]*step_n, 4, s, use_reentrant=True)\n",
    "            \n",
    "    pool[batch_idx] = torch.detach(s)  # update pool\n",
    "\n",
    "    overflow_loss = (s - s.clamp(-1.0, 1.0)).abs().mean()\n",
    "    texture_loss = 0.0\n",
    "\n",
    "    # Assuming that the batch size is 1\n",
    "    s_rgb = rgb_delta(s).squeeze(0) + col\n",
    "    imgs = torch.stack((s_rgb[:, 0, :, :], s_rgb[:, -1, :, :],\n",
    "                        s_rgb[:, :, 0, :], s_rgb[:, :, -1, :],\n",
    "                        s_rgb[:, :, :, 0], s_rgb[:, :, :, -1]), dim=0)\n",
    "    texture_loss = loss_fn(imgs)\n",
    "\n",
    "\n",
    "    loss = texture_loss + overflow_loss\n",
    "    loss_log.append(loss.item())\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if prec is torch.float16:\n",
    "            scaler.unscale_(opt)\n",
    "        for p in model.parameters():\n",
    "            p.grad /= (p.grad.norm() + 1e-8)  # normalize gradients\n",
    "        if prec is torch.float16:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            opt.step()\n",
    "        lr_sched.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    if epoch % 50 == 0:\n",
    "        clear_output()\n",
    "        plt.plot(loss_log, \".\", alpha=0.1)\n",
    "        plt.ylim(top=loss_log[0])\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        to_pil_image(imgs).show()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b14647d",
   "metadata": {},
   "source": [
    "## Visualize the NCA output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75602a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "hspacer = np.ones((res * 3 + 20, 10, 3))\n",
    "vspacer = np.ones((10, res, 3))\n",
    "with VideoWriter(fps=24, ffmpeg_params=['-pix_fmt', 'yuv420p'],\n",
    "                 bitrate='10000k') as vid, torch.no_grad():\n",
    "    s = model.seed(1, res, res, res)\n",
    "    face_images = external_faces.cpu()\n",
    "\n",
    "    for step in tqdm(range(200)):\n",
    "        with torch.autocast(device_type='cuda'):\n",
    "            z = depthwise_conv(torch.cat([s, cond], dim=1), model.filters)  # [b, 5 * chn, h, w]\n",
    "            if model.pemb:\n",
    "                z = torch.cat([z, grid[None, ...]], dim=1)\n",
    "\n",
    "            h = torch.relu(model.w1(z))\n",
    "            # Un-comment the deletes if you run out of memory\n",
    "#             del z\n",
    "            delta_s = model.w2(h)\n",
    "#             del h\n",
    "            s[:] += delta_s\n",
    "#             del delta_s\n",
    "\n",
    "        rgb_vol = col + s[0:, :3, :, :, :]        \n",
    "        rgb_ABCDEF = reorder_faces(get_outer_faces(rgb_vol[0]), \"ABCDEF\")\n",
    "        img = get_unfolded_image(rgb_ABCDEF.permute(0, 2, 3, 1).cpu().numpy())\n",
    "\n",
    "        \n",
    "        vid.add(img)\n",
    "\n",
    "    vid.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
